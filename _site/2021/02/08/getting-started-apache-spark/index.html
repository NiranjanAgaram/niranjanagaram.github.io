<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="I finally tried Apache Spark on a real project. Here&#39;s what worked, what didn&#39;t, and why I&#39;m both excited and frustrated.">
    <meta name="author" content="Niranjan Agaram">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="post">
    <meta property="og:url" content="http://localhost:4009/2021/02/08/getting-started-apache-spark/">
    <meta property="og:title" content="Getting Started with Apache Spark: Lessons from My First Project">
    <meta property="og:description" content="I finally tried Apache Spark on a real project. Here&#39;s what worked, what didn&#39;t, and why I&#39;m both excited and frustrated.">
    <meta property="og:image" content="http://localhost:4009/assets/images/og-image.svg">
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="http://localhost:4009/2021/02/08/getting-started-apache-spark/">
    <meta property="twitter:title" content="Getting Started with Apache Spark: Lessons from My First Project">
    <meta property="twitter:description" content="I finally tried Apache Spark on a real project. Here&#39;s what worked, what didn&#39;t, and why I&#39;m both excited and frustrated.">
    <meta property="twitter:image" content="http://localhost:4009/assets/images/og-image.svg">
    
    <!-- Enhanced Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Niranjan Agaram",
      "jobTitle": "AI & Data Engineering Consultant",
      "description": "10+ years evolving from data engineering to agentic AI systems across healthcare, retail, marketing, and real estate",
      "url": "http://localhost:4009",
      "sameAs": [
        "https://linkedin.com/in/niranjan-agaram",
        "https://github.com/niranjanagaram"
      ],
      "knowsAbout": [
        "Artificial Intelligence",
        "Data Engineering", 
        "Machine Learning",
        "LangChain",
        "CrewAI",
        "FastAPI",
        "Python",
        "SAS",
        "Multi-Agent Systems"
      ],
      "hasOccupation": {
        "@type": "Occupation",
        "name": "AI Consultant",
        "occupationLocation": {
          "@type": "Country",
          "name": "India"
        }
      }
    }
    </script>
    
    <title>Getting Started with Apache Spark: Lessons from My First Project</title>
    <link rel="canonical" href="http://localhost:4009/2021/02/08/getting-started-apache-spark/">
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#3b82f6">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="apple-mobile-web-app-title" content="Niranjan AI">
    <script src="/assets/js/main.js" defer></script>
    <script src="/assets/js/search.js" defer></script>
    <script src="/assets/js/performance.js" defer></script>
    
    <!-- Analytics -->
    
</head>
<body>

    <div class="bg-orb orb-1" aria-hidden="true"></div>
    <div class="bg-orb orb-2" aria-hidden="true"></div>
    
    <!-- Progress Bar -->
    <div class="reading-progress" id="reading-progress" aria-hidden="true"></div>
    
    <header role="banner">
        <div class="container">
            <h1><a href="/" aria-label="Home - Niranjan's AI Insights">Niranjan's AI Insights</a></h1>
            <p>Enterprise AI solutions, agentic systems, and intelligent automation insights from a senior data engineer</p>
            
            <!-- Search -->
            <div class="search-container">
                <input type="search" id="search-input" placeholder="Search posts..." aria-label="Search blog posts">
                <div id="search-results" class="search-results" aria-live="polite"></div>
            </div>
            
            <nav role="navigation" aria-label="Main navigation">
                <div class="nav-links">
                    <a href="/" >Home</a>
                    <a href="/posts" >Posts</a>
                    <a href="/services" >Services</a>
                    <a href="/about" >About</a>
                    <a href="/contact" >Contact</a>
                    <a href="/archive" >Archive</a>
                </div>
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle dark/light mode"></button>
            </nav>
        </div>
    </header>
    
    <main id="main-content" class="container" role="main">
        <article itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    <h1 itemprop="headline">Getting Started with Apache Spark: Lessons from My First Project</h1>
    <div class="post-meta">
      <time datetime="2021-02-08T00:00:00+05:30" itemprop="datePublished">
        February 08, 2021
      </time>
      
        <span itemprop="author" itemscope itemtype="http://schema.org/Person">
          by <span itemprop="name">Niranjan Agaram</span>
        </span>
      
      <span class="reading-time">📖 9 min read</span>
    </div>
    
    <div class="tags">
      
        <span class="tag" itemprop="keywords">spark</span>
      
        <span class="tag" itemprop="keywords">pyspark</span>
      
        <span class="tag" itemprop="keywords">big-data</span>
      
        <span class="tag" itemprop="keywords">learning</span>
      
        <span class="tag" itemprop="keywords">performance</span>
      
    </div>
    
  </header>

  <div itemprop="articleBody">
    <h1 id="getting-started-with-apache-spark-lessons-from-my-first-project">Getting Started with Apache Spark: Lessons from My First Project</h1>

<p>After months of hearing about Apache Spark at every data meetup, I finally got a chance to use it on a real project. Our patient data had grown to 5+ million records, and my trusty pandas + PostgreSQL setup was starting to struggle. Time to see what all the Spark hype was about.</p>

<h2 id="the-project">The Project</h2>

<p>We needed to analyze 3 years of patient visit data to identify readmission patterns. The dataset:</p>
<ul>
  <li><strong>5.2 million patient visits</strong></li>
  <li><strong>15 million lab results</strong></li>
  <li><strong>8 million billing records</strong></li>
  <li><strong>Total size</strong>: ~12 GB across multiple CSV files</li>
</ul>

<p>My existing Python pipeline was taking 4+ hours to process this, and our business users were getting impatient.</p>

<h2 id="setting-up-spark-harder-than-expected">Setting Up Spark (Harder Than Expected)</h2>

<p>Everyone talks about how easy Spark is to get started with. They’re lying.</p>

<h3 id="attempt-1-local-installation">Attempt 1: Local Installation</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># This looked simple enough</span>
pip <span class="nb">install </span>pyspark
</code></pre></div></div>

<p>First script:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"PatientAnalysis"</span><span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="s">"patient_visits.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Result: <code class="language-plaintext highlighter-rouge">java.lang.OutOfMemoryError</code></p>

<p>Turns out my laptop’s 8GB RAM wasn’t enough for Spark’s default settings.</p>

<h3 id="attempt-2-tuning-memory-settings">Attempt 2: Tuning Memory Settings</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
    <span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"PatientAnalysis"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.executor.memory"</span><span class="p">,</span> <span class="s">"4g"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.driver.memory"</span><span class="p">,</span> <span class="s">"2g"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.sql.adaptive.enabled"</span><span class="p">,</span> <span class="s">"true"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></div></div>

<p>Better, but still crashed on larger operations. Learned that Spark’s memory management is… complex.</p>

<h3 id="attempt-3-understanding-partitions">Attempt 3: Understanding Partitions</h3>

<p>This was my “aha” moment. Spark works with partitions, not entire datasets:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Read with explicit partitioning
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="s">"patient_visits.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>  <span class="c1"># 8 partitions for my 4-core machine
</span>
<span class="c1"># Check partition count
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Partitions: </span><span class="si">{</span><span class="n">df</span><span class="p">.</span><span class="n">rdd</span><span class="p">.</span><span class="n">getNumPartitions</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally, something that worked consistently!</p>

<h2 id="the-learning-curve">The Learning Curve</h2>

<h3 id="dataframes-vs-rdds">DataFrames vs RDDs</h3>

<p>Everyone said “use DataFrames, not RDDs.” But the DataFrame API felt weird coming from pandas:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pandas way (familiar)
</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'age'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">65</span><span class="p">][</span><span class="s">'department'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()</span>

<span class="c1"># Spark way (confusing at first)
</span><span class="n">df</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">65</span><span class="p">)</span> \
  <span class="p">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">'department'</span><span class="p">)</span> \
  <span class="p">.</span><span class="n">count</span><span class="p">()</span> \
  <span class="p">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">'count'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> \
  <span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>The Spark way is more verbose, but it’s actually more explicit about what’s happening.</p>

<h3 id="lazy-evaluation">Lazy Evaluation</h3>

<p>This concept took me a while to grasp:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This doesn't actually do anything yet
</span><span class="n">filtered_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">65</span><span class="p">)</span>
<span class="n">grouped_df</span> <span class="o">=</span> <span class="n">filtered_df</span><span class="p">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">'department'</span><span class="p">).</span><span class="n">count</span><span class="p">()</span>

<span class="c1"># Only this triggers actual computation
</span><span class="n">result</span> <span class="o">=</span> <span class="n">grouped_df</span><span class="p">.</span><span class="n">collect</span><span class="p">()</span>  <span class="c1"># or .show(), .write(), etc.
</span></code></pre></div></div>

<p>At first, this felt inefficient. Why not just do the work immediately? But once I understood query optimization, it made sense.</p>

<h2 id="my-first-real-analysis">My First Real Analysis</h2>

<p>Here’s the readmission analysis I built:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.window</span> <span class="kn">import</span> <span class="n">Window</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
    <span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"ReadmissionAnalysis"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.executor.memory"</span><span class="p">,</span> <span class="s">"4g"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="c1"># Load data
</span><span class="n">visits</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="s">"patient_visits.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">patients</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="s">"patient_demographics.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Join datasets
</span><span class="n">combined</span> <span class="o">=</span> <span class="n">visits</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">patients</span><span class="p">,</span> <span class="s">"patient_id"</span><span class="p">,</span> <span class="s">"inner"</span><span class="p">)</span>

<span class="c1"># Define window for finding next visit
</span><span class="n">window_spec</span> <span class="o">=</span> <span class="n">Window</span><span class="p">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">"patient_id"</span><span class="p">).</span><span class="n">orderBy</span><span class="p">(</span><span class="s">"admit_date"</span><span class="p">)</span>

<span class="c1"># Calculate days to next visit
</span><span class="n">with_next_visit</span> <span class="o">=</span> <span class="n">combined</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span>
    <span class="s">"next_admit_date"</span><span class="p">,</span> 
    <span class="n">lead</span><span class="p">(</span><span class="s">"admit_date"</span><span class="p">).</span><span class="n">over</span><span class="p">(</span><span class="n">window_spec</span><span class="p">)</span>
<span class="p">).</span><span class="n">withColumn</span><span class="p">(</span>
    <span class="s">"days_to_readmission"</span><span class="p">,</span>
    <span class="n">datediff</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"next_admit_date"</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">"discharge_date"</span><span class="p">))</span>
<span class="p">)</span>

<span class="c1"># Identify readmissions (within 30 days)
</span><span class="n">readmissions</span> <span class="o">=</span> <span class="n">with_next_visit</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span>
    <span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"days_to_readmission"</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> 
    <span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"days_to_readmission"</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Analysis by department
</span><span class="n">readmission_rates</span> <span class="o">=</span> <span class="n">combined</span><span class="p">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">"department"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">agg</span><span class="p">(</span>
        <span class="n">count</span><span class="p">(</span><span class="s">"*"</span><span class="p">).</span><span class="n">alias</span><span class="p">(</span><span class="s">"total_visits"</span><span class="p">),</span>
        <span class="nb">sum</span><span class="p">(</span><span class="n">when</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"days_to_readmission"</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">alias</span><span class="p">(</span><span class="s">"readmissions"</span><span class="p">)</span>
    <span class="p">).</span><span class="n">withColumn</span><span class="p">(</span>
        <span class="s">"readmission_rate"</span><span class="p">,</span>
        <span class="nb">round</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"readmissions"</span><span class="p">)</span> <span class="o">/</span> <span class="n">col</span><span class="p">(</span><span class="s">"total_visits"</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>

<span class="n">readmission_rates</span><span class="p">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">desc</span><span class="p">(</span><span class="s">"readmission_rate"</span><span class="p">)).</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>This analysis took 12 minutes in Spark vs. 3+ hours in my pandas version. I was impressed.</p>

<h2 id="what-worked-well">What Worked Well</h2>

<h3 id="1-performance-on-large-datasets">1. Performance on Large Datasets</h3>
<p>Once I got the configuration right, Spark was significantly faster than pandas for operations on large datasets.</p>

<h3 id="2-sql-interface">2. SQL Interface</h3>
<p>Being able to use SQL was great:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Register as temp view
</span><span class="n">combined</span><span class="p">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s">"patient_visits"</span><span class="p">)</span>

<span class="c1"># Use familiar SQL
</span><span class="n">result</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"""
    SELECT department, 
           COUNT(*) as total_visits,
           AVG(length_of_stay) as avg_los
    FROM patient_visits 
    WHERE admit_date &gt;= '2020-01-01'
    GROUP BY department
    ORDER BY avg_los DESC
"""</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="3-built-in-optimizations">3. Built-in Optimizations</h3>
<p>The Catalyst optimizer actually made some of my queries faster than I expected. It’s like having a DBA optimize your queries automatically.</p>

<h2 id="what-frustrated-me">What Frustrated Me</h2>

<h3 id="1-error-messages">1. Error Messages</h3>
<p>Spark error messages are terrible. A simple typo gives you a 50-line Java stack trace that tells you nothing useful.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>py4j.protocol.Py4JJavaError: An error occurred while calling o123.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: 
Task 0 in stage 12.0 failed 1 times, most recent failure: 
Lost task 0.0 in stage 12.0 (TID 12, localhost, executor driver): 
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
...
</code></pre></div></div>

<p>Compare this to pandas: <code class="language-plaintext highlighter-rouge">KeyError: 'column_name'</code>. Much clearer.</p>

<h3 id="2-memory-tuning">2. Memory Tuning</h3>
<p>Getting memory settings right is still black magic to me. Too little memory = crashes. Too much memory = slow startup. The sweet spot is hard to find.</p>

<h3 id="3-development-workflow">3. Development Workflow</h3>
<p>The feedback loop is slower than pandas. Every operation takes a few seconds to start up, which makes interactive development painful.</p>

<h2 id="performance-comparison">Performance Comparison</h2>

<p>I ran the same readmission analysis on both platforms:</p>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>Pandas</th>
      <th>Spark</th>
      <th>Dataset Size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Data Loading</td>
      <td>45s</td>
      <td>12s</td>
      <td>5M records</td>
    </tr>
    <tr>
      <td>Filtering</td>
      <td>8s</td>
      <td>3s</td>
      <td>5M records</td>
    </tr>
    <tr>
      <td>Grouping/Aggregation</td>
      <td>25s</td>
      <td>7s</td>
      <td>5M records</td>
    </tr>
    <tr>
      <td>Window Functions</td>
      <td>180s</td>
      <td>15s</td>
      <td>5M records</td>
    </tr>
    <tr>
      <td><strong>Total Runtime</strong></td>
      <td><strong>258s</strong></td>
      <td><strong>37s</strong></td>
      <td><strong>5M records</strong></td>
    </tr>
  </tbody>
</table>

<p>Spark was 7x faster overall, but the difference was most dramatic for complex operations like window functions.</p>

<h2 id="lessons-learned">Lessons Learned</h2>

<h3 id="1-spark-isnt-always-better">1. Spark Isn’t Always Better</h3>
<p>For small datasets (&lt; 1M records), pandas is often faster and definitely easier to work with. Spark’s overhead isn’t worth it.</p>

<h3 id="2-partitioning-matters">2. Partitioning Matters</h3>
<p>Understanding how your data is partitioned is crucial for performance:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check partition distribution
</span><span class="n">df</span><span class="p">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">spark_partition_id</span><span class="p">()).</span><span class="n">count</span><span class="p">().</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Repartition if needed
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">repartition</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"department"</span><span class="p">))</span>  <span class="c1"># Partition by department
</span></code></pre></div></div>

<h3 id="3-caching-is-important">3. Caching Is Important</h3>
<p>For iterative operations, caching intermediate results makes a huge difference:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Cache frequently used datasets
</span><span class="n">filtered_data</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">18</span><span class="p">).</span><span class="n">cache</span><span class="p">()</span>

<span class="c1"># Use it multiple times without recomputing
</span><span class="n">result1</span> <span class="o">=</span> <span class="n">filtered_data</span><span class="p">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">"department"</span><span class="p">).</span><span class="n">count</span><span class="p">()</span>
<span class="n">result2</span> <span class="o">=</span> <span class="n">filtered_data</span><span class="p">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">"gender"</span><span class="p">).</span><span class="n">count</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="4-start-simple">4. Start Simple</h3>
<p>My first Spark script tried to do everything at once. Breaking it into smaller, testable pieces made debugging much easier.</p>

<h2 id="when-to-use-spark">When to Use Spark</h2>

<p>Based on my experience:</p>

<p><strong>Use Spark when:</strong></p>
<ul>
  <li>Dataset &gt; 2-3 GB</li>
  <li>Complex transformations (joins, window functions)</li>
  <li>Need to scale beyond single machine</li>
  <li>Working with multiple data sources</li>
</ul>

<p><strong>Stick with pandas when:</strong></p>
<ul>
  <li>Dataset &lt; 1 GB</li>
  <li>Simple operations</li>
  <li>Interactive analysis</li>
  <li>Rapid prototyping</li>
</ul>

<h2 id="whats-next">What’s Next</h2>

<p>I’m planning to explore:</p>
<ol>
  <li><strong>Spark on cloud platforms</strong> (AWS EMR, Databricks)</li>
  <li><strong>Streaming with Spark</strong> (real-time data processing)</li>
  <li><strong>MLlib</strong> (Spark’s machine learning library)</li>
  <li><strong>Delta Lake</strong> (for better data management)</li>
</ol>

<h2 id="for-others-getting-started">For Others Getting Started</h2>

<p><strong>Don’t expect it to be easy.</strong> Spark has a learning curve, especially if you’re coming from pandas.</p>

<p><strong>Start with the DataFrame API.</strong> Ignore RDDs unless you have a specific need.</p>

<p><strong>Learn about partitioning early.</strong> It affects everything in Spark.</p>

<p><strong>Use the Spark UI.</strong> It’s invaluable for understanding what’s actually happening: <code class="language-plaintext highlighter-rouge">http://localhost:4040</code></p>

<p><strong>Practice on real data.</strong> Toy examples don’t show you the real challenges.</p>

<h2 id="the-bottom-line">The Bottom Line</h2>

<p>Spark is powerful, but it’s not magic. It solved my performance problems, but introduced new complexity. For large-scale data processing, it’s worth the learning curve. For everything else, pandas is still my go-to.</p>

<hr />

<p><em>Next post: I’m going to compare AWS and Azure for data workloads. My company is considering a cloud migration, and I volunteered to do the research. Wish me luck navigating enterprise procurement!</em></p>

  </div>

  <!-- Social Share Buttons -->
  <div class="share-buttons">
    <h4>Share this post:</h4>
    <a href="https://twitter.com/intent/tweet?text=Getting%20Started%20with%20Apache%20Spark:%20Lessons%20from%20My%20First%20Project&url=http://localhost:4009/2021/02/08/getting-started-apache-spark/" 
       class="share-button" target="_blank" rel="noopener" aria-label="Share on Twitter">
      🐦 Twitter
    </a>
    <a href="https://www.linkedin.com/sharing/share-offsite/?url=http://localhost:4009/2021/02/08/getting-started-apache-spark/" 
       class="share-button" target="_blank" rel="noopener" aria-label="Share on LinkedIn">
      💼 LinkedIn
    </a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4009/2021/02/08/getting-started-apache-spark/" 
       class="share-button" target="_blank" rel="noopener" aria-label="Share on Facebook">
      📘 Facebook
    </a>
    <button class="share-button" onclick="copyToClipboard('http://localhost:4009/2021/02/08/getting-started-apache-spark/')" aria-label="Copy link">
      🔗 Copy Link
    </button>
  </div>

  <!-- AI-Powered Recommendations -->
  <div class="recommendations">
    <h3>Recommended for you</h3>
    <div class="recommended-posts">
      
      
      
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2025/08/20/ai-model-selection-strategies/">AI Model Selection: When to Use GPT vs Claude vs Open Source Models</a></h4>
          <div class="post-meta">August 20, 2025</div>
          <div class="post-excerpt">After using different AI models for 8 months, I've learned when each one shines. Here's my practical guide to choosing...</div>
        </div>
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2025/06/15/voice-ai-integration-streamlit/">Voice AI Integration: Adding Speech to My AI Applications with Streamlit</a></h4>
          <div class="post-meta">June 15, 2025</div>
          <div class="post-excerpt">I added voice capabilities to my AI applications. Here's how I integrated speech-to-text and text-to-speech with Streamlit for hands-free AI...</div>
        </div>
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2025/04/10/building-multi-agent-systems-crewai/">Building Multi-Agent Systems with CrewAI: Beyond Single AI Assistants</a></h4>
          <div class="post-meta">April 10, 2025</div>
          <div class="post-excerpt">I built my first multi-agent system using CrewAI. Here's what I learned about orchestrating multiple AI agents to solve complex...</div>
        </div>
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2025/02/20/chain-of-thought-prompting-business-logic/">Chain-of-Thought Prompting: Improving AI Reasoning in Business Applications</a></h4>
          <div class="post-meta">February 20, 2025</div>
          <div class="post-excerpt">Can we get LLMs to reason through complex business rules reliably? My experiments with chain-of-thought prompting for healthcare protocols.</div>
        </div>
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2025/01/15/advanced-prompt-engineering-techniques/">Advanced Prompt Engineering: Techniques I've Learned from 6 Months with LLMs</a></h4>
          <div class="post-meta">January 15, 2025</div>
          <div class="post-excerpt">After 6 months of working with LLMs daily, I've discovered prompt engineering is both an art and a science. Here...</div>
        </div>
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2024/12/15/agentic-ai-customer-service-automation/">Building an Agentic AI Customer Service System: A Complete Case Study</a></h4>
          <div class="post-meta">December 15, 2024</div>
          <div class="post-excerpt">How I built a multi-agent customer service system that reduced response time by 85% and improved satisfaction scores by 40%...</div>
        </div>
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2024/10/20/getting-started-langchain-rag/">Building RAG Systems: My Journey with LangChain</a></h4>
          <div class="post-meta">October 20, 2024</div>
          <div class="post-excerpt">After months of hearing about RAG and LangChain, I finally built my first retrieval-augmented generation system. Here's what I learned....</div>
        </div>
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2024/09/10/ai-powered-data-quality-monitoring/">AI-Powered Data Quality Monitoring: The Future of Data Reliability</a></h4>
          <div class="post-meta">September 10, 2024</div>
          <div class="post-excerpt">Discover how artificial intelligence is revolutionizing data quality monitoring with automated anomaly detection, intelligent alerting, and predictive data health insights....</div>
        </div>
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2024/06/20/advanced-kafka-streaming-patterns/">Advanced Kafka Streaming Patterns for Real-Time Analytics</a></h4>
          <div class="post-meta">June 20, 2024</div>
          <div class="post-excerpt">Explore advanced Apache Kafka streaming patterns including exactly-once processing, windowing operations, and complex event processing for building robust real-time analytics...</div>
        </div>
      
        <div class="post-card">
          <div class="post-image">💡</div>
          <h4><a href="/2024/03/15/building-enterprise-ai-customer-support-system/">Building an Enterprise AI Customer Support System: From Concept to $485K Annual Savings</a></h4>
          <div class="post-meta">March 15, 2024</div>
          <div class="post-excerpt">How I built a Fortune 500-grade multi-agent customer support system using 100% free technologies, achieving 85% faster response times and...</div>
        </div>
      
    </div>
  </div>

  <!-- Comments Section -->
  
</article>

<script>
  function copyToClipboard(text) {
    navigator.clipboard.writeText(text).then(() => {
      alert('Link copied to clipboard!');
    });
  }
</script>
    </main>
    
    <!-- WhatsApp Chat Widget -->
    <div class="whatsapp-chat" id="whatsapp-chat">
        <div class="chat-bubble">
            <svg viewBox="0 0 24 24" fill="white">
                <path d="M17.472 14.382c-.297-.149-1.758-.867-2.03-.967-.273-.099-.471-.148-.67.15-.197.297-.767.966-.94 1.164-.173.199-.347.223-.644.075-.297-.15-1.255-.463-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.298-.347.446-.52.149-.174.198-.298.298-.497.099-.198.05-.371-.025-.52-.075-.149-.669-1.612-.916-2.207-.242-.579-.487-.5-.669-.51-.173-.008-.371-.01-.57-.01-.198 0-.52.074-.792.372-.272.297-1.04 1.016-1.04 2.479 0 1.462 1.065 2.875 1.213 3.074.149.198 2.096 3.2 5.077 4.487.709.306 1.262.489 1.694.625.712.227 1.36.195 1.871.118.571-.085 1.758-.719 2.006-1.413.248-.694.248-1.289.173-1.413-.074-.124-.272-.198-.57-.347m-5.421 7.403h-.004a9.87 9.87 0 01-5.031-1.378l-.361-.214-3.741.982.998-3.648-.235-.374a9.86 9.86 0 01-1.51-5.26c.001-5.45 4.436-9.884 9.888-9.884 2.64 0 5.122 1.03 6.988 2.898a9.825 9.825 0 012.893 6.994c-.003 5.45-4.437 9.884-9.885 9.884m8.413-18.297A11.815 11.815 0 0012.05 0C5.495 0 .16 5.335.157 11.892c0 2.096.547 4.142 1.588 5.945L.057 24l6.305-1.654a11.882 11.882 0 005.683 1.448h.005c6.554 0 11.89-5.335 11.893-11.893A11.821 11.821 0 0020.885 3.488"/>
            </svg>
        </div>
        <div class="chat-tooltip">Chat with me on WhatsApp!</div>
    </div>

    <footer role="contentinfo">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>Connect</h4>
                    <div class="social-links">
                        <a href="https://github.com/niranjanagaram" aria-label="GitHub Profile">GitHub</a>
                        <a href="https://linkedin.com/in/niranjan-agaram" aria-label="LinkedIn Profile">LinkedIn</a>
                        <a href="mailto:niranjan@example.com" aria-label="Email Contact">Email</a>
                    </div>
                </div>
                <div class="footer-section">
                    <h4>Subscribe</h4>
                    <a href="/feed.xml" aria-label="RSS Feed">RSS Feed</a>
                </div>
            </div>
            <p>&copy; 2025 Niranjan's AI Insights. All rights reserved.</p>
        </div>
    </footer>
    
    <!-- Service Worker -->
    <script>
        if ('serviceWorker' in navigator) {
            navigator.serviceWorker.register('/sw.js');
        }
        
        // Modern Theme Toggle
        const themeToggle = document.getElementById('theme-toggle');
        const currentTheme = localStorage.getItem('theme') || 'dark';
        
        document.documentElement.setAttribute('data-theme', currentTheme);
        
        themeToggle.addEventListener('click', () => {
            const theme = document.documentElement.getAttribute('data-theme');
            const newTheme = theme === 'dark' ? 'light' : 'dark';
            document.documentElement.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
        });
        
        // Reading Progress
        window.addEventListener('scroll', () => {
            const progress = document.getElementById('reading-progress');
            const scrolled = (window.scrollY / (document.body.scrollHeight - window.innerHeight)) * 100;
            progress.style.width = scrolled + '%';
        });
        

    </script>
</body>
</html>